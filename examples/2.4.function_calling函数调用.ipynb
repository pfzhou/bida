{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 函数调用function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 框架启动时自动从functions目录下加载所有json文件（名字是 \"_\" 开始的会被忽略）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_weather',\n",
       "  'description': 'Get the current weather in a given location',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'location': {'type': 'string',\n",
       "     'description': 'The city and state, e.g. San Francisco, CA'},\n",
       "    'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}},\n",
       "   'required': ['location']}},\n",
       " {'name': 'search_google',\n",
       "  'description': 'If the required information cannot be found, use Google search online',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'search content'}},\n",
       "   'required': ['query']}},\n",
       " {'name': 'search_baidu',\n",
       "  'description': '如果查不到答案需要联网搜索且google无法使用时，使用百度搜索',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'search content'}},\n",
       "   'required': ['query']}}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bida import ManagementFunctions\n",
    "ManagementFunctions.get_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查询城市天气，会自动调用weather函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数调用中：{'content': None, 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Beijing\"\\n}'}, 'role': 'assistant'}\n",
      "......\n",
      "北京的天气目前是晴天，温度为26摄氏度，有风。"
     ]
    }
   ],
   "source": [
    "from bida import ChatLLM\n",
    "from bida import ManagementFunctions\n",
    "\n",
    "llm = ChatLLM(model_type='openai', model_name='3.5-0613')   # 目前只有OpenAI发布的0613版及之后的模型支持function call\n",
    "\n",
    "def my_stream_process_data(data):\n",
    "    if isinstance(data, str):           \n",
    "        print(data+'', end=\"\", flush=True)\n",
    "    else:\n",
    "        print(f'函数调用中：{data}\\n......')    # 函数被调用时返回的是对象（其他时间返回的是字符串），可以将对象转为更加友好的方式显示给用户\n",
    "\n",
    "message = '北京的天气？'\n",
    "result = llm.chat(\n",
    "    prompt=message, \n",
    "    stream_callback=my_stream_process_data,\n",
    "    functions=ManagementFunctions.get_functions(),  # 添加自定义函数\n",
    "    function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '北京的天气？'},\n",
       " {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'function_call': {'name': 'get_current_weather',\n",
       "   'arguments': '{\\n  \"location\": \"Beijing\"\\n}'}},\n",
       " {'role': 'function',\n",
       "  'content': '{\"location\": \"Beijing\", \"temperature\": \"26\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}',\n",
       "  'name': 'get_current_weather'},\n",
       " {'role': 'assistant', 'content': '北京的天气目前是晴天，温度为26摄氏度，有风。'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印聊天历史\n",
    "llm.conversation.history_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用迭代器方式的模型调用效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数调用中：{'content': None, 'function_call': {'name': 'search_google', 'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}, 'role': 'assistant'}\n",
      "......\n",
      "{'content': None, 'function_call': {'name': 'search_google', 'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}, 'role': 'assistant'}函数调用中：{'content': None, 'function_call': {'name': 'search_baidu', 'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}, 'role': 'assistant'}\n",
      "......\n",
      "{'content': None, 'function_call': {'name': 'search_baidu', 'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}, 'role': 'assistant'}很很抱抱歉歉，，我我无无法法找找到到20220266世世界界杯杯的的举举办办地地。。你你可以可以尝尝试试在在互互联联网网上上搜索搜索相关相关信息信息。。"
     ]
    }
   ],
   "source": [
    "from bida import ChatLLM\n",
    "from bida import ManagementFunctions\n",
    "def my_stream_process_data(data):\n",
    "    if isinstance(data, str):           \n",
    "        print(data+'', end=\"\", flush=True)\n",
    "    else:\n",
    "        print(f'函数调用中：{data}\\n......')\n",
    "\n",
    "llm = ChatLLM(model_type='openai')\n",
    "def callllm():\n",
    "    message = '2026世界杯在哪里？'\n",
    "    for partial_message in llm.achat(\n",
    "        prompt=message, \n",
    "        stream_callback=my_stream_process_data,\n",
    "        functions=ManagementFunctions.get_functions(),\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "        increment=True):     # 改为输出增量\n",
    "        yield partial_message\n",
    "for content in callllm():\n",
    "    if isinstance(content, str):           \n",
    "        print(content+'', end=\"\", flush=True)\n",
    "    else:\n",
    "        print(f'@函数调用中：{content}\\n......')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，因为google function没有查到，gpt再次调用baidu的function进行了查询，本次查询实际上产生了3次模型调用\n",
    "1. user: '2026世界杯在哪里？', assert：'使用google function'\n",
    "2. user：'google function的结果'，assert: '使用baidu function'\n",
    "3. user: 'baidu function的结果', assert: '最终回复'\n",
    "\n",
    "下面是详细的记录："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '2026世界杯在哪里？'},\n",
       " {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'function_call': {'name': 'search_google',\n",
       "   'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}},\n",
       " {'role': 'function',\n",
       "  'content': '{\"query\": \"2026\\\\u4e16\\\\u754c\\\\u676f\\\\u4e3e\\\\u529e\\\\u5730\", \"result\": \"No relevant information found.\"}',\n",
       "  'name': 'search_google'},\n",
       " {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'function_call': {'name': 'search_baidu',\n",
       "   'arguments': '{\\n  \"query\": \"2026世界杯举办地\"\\n}'}},\n",
       " {'role': 'function',\n",
       "  'content': '{\"query\": \"2026\\\\u4e16\\\\u754c\\\\u676f\\\\u4e3e\\\\u529e\\\\u5730\", \"result\": \"\\\\u7b54\\\\u6848\\\\u662f\\\\uff1a\\\\u6211\\\\u4e5f\\\\u4e0d\\\\u77e5\\\\u9053\\\\uff0c\\\\u54c8\\\\u54c8\\\\u54c8\\\\uff0c\\\\u8fd8\\\\u662f\\\\u6211\\\\u66f4\\\\u61c2\\\\u4e2d\\\\u6587\\\\u5427~~~\"}',\n",
       "  'name': 'search_baidu'},\n",
       " {'role': 'assistant', 'content': '很抱歉，我无法找到2026世界杯的举办地。你可以尝试在互联网上搜索相关信息。'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印聊天历史\n",
    "llm.conversation.history_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 与gradio结合后的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果没有安装gradio，请先执行下面代码安装\n",
    "!pip install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from bida import ManagementFunctions\n",
    "from bida import ChatLLM\n",
    "\n",
    "\n",
    "llm = ChatLLM(model_type='openai', model_name='3.5-0613')\n",
    "\n",
    "def my_stream_process_data(content):            \n",
    "    if isinstance(content, str):           \n",
    "        pass\n",
    "    else:\n",
    "        print(f'@函数调用中：{content}\\n......')\n",
    "\n",
    "def predict(message, history):\n",
    "    for partial_message in llm.achat(\n",
    "        prompt=message, \n",
    "        # stream_callback=my_stream_process_data,\n",
    "        functions=ManagementFunctions.get_functions(),\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "        ):\n",
    "        yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).queue().launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
